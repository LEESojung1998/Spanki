#!/usr/bin/env python
# encoding: utf-8
"""
sim_transcripts

Simulates reads from transcript sequence.  
Introduces error according to models built from
read data, or default random models.
"""
from __future__ import division 

import string
import random
import itertools

import re
import sys
import argparse
import pysam
import collections
import math
import os
import csv

import numpy as np
from pyfasta import Fasta

from random import choice
from datetime import datetime, date


# Custom modules to import:
import spanky.spanky_parse_utils as spanky_parse_utils
import spanky.spanky_utils as spanky_utils
import spanky.sim_models as models


# Biopython:
from Bio.Seq import Seq
from Bio.Alphabet import IUPAC


def isodd(num):
	# See here
	#http://stackoverflow.com/questions/1089936/even-and-odd-number
	# For exp why it is most efficient this way
	return num & 1 and True or False

def getTxSequence(DICT,f):
 	"""
 	Prints the intron sequence data for each join
 	"""
 	# STAB has annotation information
	chr = DICT['chr']
	start = DICT['start']
	mypos_spliced = start
	'''
	Incoming start is zero-based
	'''
	mypos_unspliced = start
	strand = DICT['strand']
	chain = DICT['chain']
	txseq_spliced = ""
	txseq_unspliced = ""
	for chunk in chain:
		chunklength = chunk[1]
		'''
		The sequence call is zero-based
		see:
		http://pypi.python.org/pypi/pyfasta/
		Then a seq object is made using biopython
		see:
		http://biopython.org/DIST/docs/tutorial/Tutorial.html
		'''
		txseq_unspliced += Seq(f[chr][mypos_unspliced:mypos_unspliced + chunklength], IUPAC.unambiguous_dna)
		if (chunk[0] == 0):
			txseq_spliced += Seq(f[chr][mypos_unspliced:mypos_unspliced + chunklength], IUPAC.unambiguous_dna)
			#mypos_spliced += chunklength
		mypos_unspliced += chunklength
	if strand == '-':
		txseq_spliced = txseq_spliced.reverse_complement()
		txseq_unspliced = txseq_unspliced.reverse_complement()
	return txseq_spliced, txseq_unspliced


def cigarToTuple(cigar):
	'''
	Takes cigar like 7M3N10M
	and makes a tuple
	'''
	counter = ""
	cigartuple = []
	for char in cigar:
		if char.isdigit():
			counter += str(char)
		elif char == "M":
			cigartuple.append([0,int(counter)])
			counter = ""
		elif char == "N":
			cigartuple.append([3,int(counter)])
			counter = ""
	
	return cigartuple
	

def getReadSequence(start, chr, strand, cigar, f):
 	"""
 	Prints the intron sequence data for each join
 	"""
 	# STAB has annotation information
	cigartuple = cigarToTuple(cigar)
	chain = cigartuple
	readseq = ""
	mypos = start
	for chunk in chain:
		chunklength = chunk[1]
		'''
		The sequence call is zero-based
		see:
		http://pypi.python.org/pypi/pyfasta/
		Then a seq object is made using biopython
		see:
		http://biopython.org/DIST/docs/tutorial/Tutorial.html
		'''
		if (chunk[0] == 0):
			readseq += Seq(f[chr][mypos:mypos + chunklength], IUPAC.unambiguous_dna)
		mypos += chunklength	
	if strand == '-':
		readseq = readseq.reverse_complement()

	return readseq


def find(strng, ch, start=0):
	# from:
	#http://openbookproject.net/thinkCSpy/ch07.html
	index = start
	while index < len(strng):
		if strng[index] == ch:
			return index
		index += 1
	return -1



def makePreRNAindex(cigar,start):
	spl2pre = []
	counter = 0
	for x in cigar:		
		prepos_toadd = range(counter,counter + int(x[1]))  # index of pos in parent tx
		counter += int(x[1])
		if x[0] == 0:
			spl2pre.extend(prepos_toadd)	
	return spl2pre

def genomeCoordString(cigar,start):
	pre2genome = [start]
	spl2pre = []
	counter = 0
	matchcounter = 0
	#print cigar
	#print len(cigar)
	totallength = 0
	for x in cigar:		
		genomepos_toadd = range(pre2genome[-1] + 1,pre2genome[-1] + int(x[1]) + 1) # index of genomic pos
		prepos_toadd = range(counter,counter + int(x[1]))  # index of pos in parent tx
		counter += int(x[1])
		pre2genome.extend(genomepos_toadd)
		if x[0] == 0:
			spl2pre.extend(prepos_toadd)	
	return pre2genome, spl2pre

def explodingCigar(cigar):
	cigarstring = ""
	for x in cigar:
		if (x[0] == 0):
			cigarstring += "M" * int(x[1])
		elif (x[0] == 3):
			cigarstring += "N" * int(x[1])
		else:
			print "cigar error"
	return cigarstring
	
def implodeCigar(cigar):
	cigarstring = ""
	cigar = cigar.replace("MN", "M,N");
	cigar = cigar.replace("NM", "N,M");
	foo = re.split(',',cigar)
	#print foo
	for x in foo:
		cigarstring += str(len(x)) + x[0]
	return cigarstring
	
def weighted_choice(weights):
	# See:
	#http://eli.thegreenplace.net/2010/01/22/weighted-random-generation-in-python/
	totals = np.cumsum(weights)
	norm = totals[-1]
	throw = np.random.rand()*norm
	return np.searchsorted(totals, throw)


def sam2junc(cigartuple,strand,chr,start):
	if (len(cigartuple) > 1):
		offset = start
		gaps = []
		matches = []
		addnextmatch = 0
		addtomatch = 0
		for i in cigartuple:
			# Iterate over each entry in cigar
			# matches should be appended 
			if (i[0] == 0):
				if addnextmatch > 0:
					matches[-1] = matches[-1] + i[1] + addtomatch
					addnextmatch = 0
					addtomatch = 0
				else: matches.append(i[1])
			elif (i[0] == 3):
				gaps.append(i[1])
			elif (i[0] == 1):
				addnextmatch += 1
				addtomatch = 0
			elif (i[0] == 2):
				addnextmatch += 1
				addtomatch = i[1]
		if len(gaps) > 0:
			for i in range(len(gaps)):
				junc_left_side = start + matches[i] + 1
				junc_right_side = start + matches[i] + gaps[i] 
				juncid = str(chr) + ":" + str(junc_left_side) + "_" + str(junc_right_side) + ":" + str(strand)
				start = start + matches[i] + gaps[i]
	return juncid


def simRead(mytx,readnum,myseq,mycigar,pre2genome,strandidx, mybp, mmprob, mmposprob, TX, spl2pre, f, refjuncs, qualscore, mmqual):

	randstart = random.randint(0, len(myseq) - mybp - 1)

	read = myseq[randstart:randstart + mybp]
	
	# Get position in parent transcript and in genome
	prestart = spl2pre[randstart]
	preend = spl2pre[randstart + mybp]

	genomicstart = pre2genome[prestart]
	mystart = genomicstart
	genomicend = pre2genome[preend]

	readcigar = mycigar[prestart:preend]
	
	# randomly flip strand, but remember original strand
	mystrand = strandidx[0]
	flip = random.randrange(2)
	if (flip > 0): 
		mystrand = strandidx[1]
	
	jsr = 0
	if len(mycigar) > mybp:
		newcigar = implodeCigar(readcigar)
		jsr += 1
	else:
		newcigar = str(mybp) + "M"

	if mystrand == "-":
		read = read.reverse_complement()

	if bool(jsr):	
		checkread = getReadSequence(mystart, TX[mytx]['chr'], mystrand, newcigar, f)
		if str(read) != str(checkread):
			print read
			print checkread
			quit()

	# Get counts of simmed reads for each junc
	if bool(jsr): # If it is a junction spanning read
		cigartuple = cigarToTuple(newcigar)
		#print cigartuple
		#print genomicstart
		mystart = genomicstart
		while len(cigartuple) > 1:
			mytuple = cigartuple[0:3]
			cigartuple = cigartuple[2:]
			minoverhang = mybp
			#print "Getting counts", newcigar, mytuple
			#Zero anchor example:
			#Getting counts 76M23846N [[0, 76], [3, 23846]]
			for i in mytuple:
				if i[0] == 0:
					if i[1] == mybp: minoverhang = 0
					if minoverhang > i[1]: minoverhang = i[1]
			juncid = sam2junc(mytuple,strandidx[0],TX[mytx]['chr'],mystart)
			if juncid in refjuncs:
				if minoverhang > 0:
					JUNCRESULTS[juncid]['totalcov'] += 1
					if minoverhang >= 8:
						JUNCRESULTS[juncid]['effectivecov'] += 1
				else:
					newcigar = str(mybp) + "M"
			else:
				print juncid, "simmed, NOT in annotation"
				quit()
			mystart = mystart + mytuple[0][1] + mytuple[1][1]


	pretty_readnum = "%05d" % readnum
	if mystrand == "+":
		pretty_strand = "plus"
	else:
		pretty_strand = "minus"
		read = read.reverse_complement()

	### Mismatches
	bases = ['A','G','C','T']
	'''
	Uses the model to determine how many total mismatches to induce
	'''
	mmnum = weighted_choice(mmprob)
	#print "Will induce ", mmnum, "mismatches"
	for mm in range(mmnum):
		'''
		Uses the model to determine where to put mismatches
		'''
		mmpos = weighted_choice(mmposprob)
		#print "Will put mismatch at position", mmpos, "read is length", len(read)
		oldbase = read[mmpos]
		newbase = oldbase
		while oldbase == newbase:
			newbase = choice(bases)
		read = read[0:mmpos] + newbase + read[mmpos+1:]
		# substitute lower quality in position where mismatch is in
		qualscore = qualscore[0:mmpos] + mmqual + qualscore[mmpos+1:]

	# Note that in SAM file, read is presented as strand it aligns to, not as the read 
	# comes off the machine

	id = mytx + "_" + str(pretty_readnum) + "_" + TX[mytx]['chr'] + ":" + str(genomicstart + 1) + ":" + pretty_strand + "_NM" + str(mmnum) + "_" + newcigar + "#0/1"
	readid = "@" + id

	readtuple = [readid,read,"+",qualscore]

	samline = []
	samline.append(id)
	if mystrand == "+":
		samline.append("0")
	else:
		samline.append("16")
	samline.append(TX[mytx]['chr'])
	samline.append(str(genomicstart + 1))
	samline.append("255")
	samline.append(newcigar)
	samline.append("*")
	samline.append("0")
	samline.append("0")
	samline.append(str(read))
	samline.append(qualscore)
	samline.append("NM:i:" + str(mmnum))
	samline.append("XS:A:" + strandidx[0])
	samline.append("NH:i:1")
	return readtuple, samline

def simRead_weightstart(mytx,readnum,myseq,mycigar,pre2genome,strandidx, mybp, mmprob, mmposprob, TX, spl2pre, f, refjuncs, qualscore, mmqual,startweights):

	#randstart = random.randint(0, len(myseq) - mybp - 1)
	
	randstart = weighted_choice(startweights[0:len(myseq) - mybp - 1])
	#print randstart, startweights[randstart]
	#print "length of seq is ", len(myseq)
	#print "my start is ", randstart
	

	read = myseq[randstart:randstart + mybp]
	
	# Get position in parent transcript and in genome
	prestart = spl2pre[randstart]
	preend = spl2pre[randstart + mybp]

	genomicstart = pre2genome[prestart]
	mystart = genomicstart
	genomicend = pre2genome[preend]

	readcigar = mycigar[prestart:preend]
	# randomly flip strand, but remember original strand
	mystrand = strandidx[0]
	flip = random.randrange(2)
	if (flip > 0): 
		mystrand = strandidx[1]
	
	jsr = 0
	if len(mycigar) > mybp:
		newcigar = implodeCigar(readcigar)
		jsr += 1
	else:
		newcigar = "76M"

	if mystrand == "-":
		read = read.reverse_complement()

	if bool(jsr):	
		checkread = getReadSequence(mystart, TX[mytx]['chr'], mystrand, newcigar, f)
		if str(read) != str(checkread):
			print read
			print checkread
			quit()

	# Get counts of simmed reads for each junc
	if bool(jsr): # If it is a junction spanning read
		cigartuple = cigarToTuple(newcigar)
		#print cigartuple
		#print genomicstart
		mystart = genomicstart
		while len(cigartuple) > 1:
			mytuple = cigartuple[0:3]
			cigartuple = cigartuple[2:]
			minoverhang = mybp
			#print "Getting counts", newcigar, mytuple
			#Zero anchor example:
			#Getting counts 76M23846N [[0, 76], [3, 23846]]
			for i in mytuple:
				if i[0] == 0:
					if i[1] == mybp: minoverhang = 0
					if minoverhang > i[1]: minoverhang = i[1]
			juncid = sam2junc(mytuple,strandidx[0],TX[mytx]['chr'],mystart)
			if juncid in refjuncs:
				if minoverhang > 0:
					JUNCRESULTS[juncid]['totalcov'] += 1
					if minoverhang >= 8:
						JUNCRESULTS[juncid]['effectivecov'] += 1
				else:
					newcigar = str(mybp) + "M"
			else:
				print juncid, "simmed, NOT in annotation"
				quit()
			mystart = mystart + mytuple[0][1] + mytuple[1][1]

	pretty_readnum = "%05d" % readnum
	if mystrand == "+":
		pretty_strand = "plus"
	else:
		pretty_strand = "minus"
		read = read.reverse_complement()

	### Mismatches
	bases = ['A','G','C','T']
	mmnum = weighted_choice(mmprob)
	for mm in range(mmnum):
		mmpos = weighted_choice(mmposprob)
		oldbase = read[mmpos]
		newbase = oldbase
		while oldbase == newbase:
			newbase = choice(bases)
		read = read[0:mmpos] + newbase + read[mmpos+1:]
		# substitute lower quality in position where mismatch is in
		qualscore = qualscore[0:mmpos] + mmqual + qualscore[mmpos+1:]

	# Note that in SAM file, read is presented as strand it aligns to, not as the read 
	# comes off the machine

	id = mytx + "_" + str(pretty_readnum) + "_" + TX[mytx]['chr'] + ":" + str(genomicstart + 1) + ":" + pretty_strand + "_NM" + str(mmnum) + "_" + newcigar + "#0/1"
	readid = "@" + id

	readtuple = [readid,read,"+",qualscore]

	samline = []
	samline.append(id)
	if mystrand == "+":
		samline.append("0")
	else:
		samline.append("16")
	samline.append(TX[mytx]['chr'])
	samline.append(str(genomicstart + 1))
	samline.append("255")
	samline.append(newcigar)
	samline.append("*")
	samline.append("0")
	samline.append("0")
	samline.append(str(read))
	samline.append(qualscore)
	samline.append("NM:i:" + str(mmnum))
	samline.append("XS:A:" + strandidx[0])
	samline.append("NH:i:1")
	
	return readtuple, samline

def getStartweights(myseq,mers,seqweights):
	#print myseq
	startweights = []
	for i in range(0,len(myseq) - mers + 1):
		mymer = myseq[i:i+mers]
		"""
		Account for cases with IUPAC ambig. codes
		"""
		try:
			startweights.append(seqweights[mymer])
		except:
			startweights.append(1)
			
	return startweights


def reference_transcript_dict(samfile):
	TX = collections.defaultdict(lambda : collections.defaultdict(dict))
	#print >> sys.stderr, "[%s] Iterating through flattedned ref file %s" % (right_now(), output_dir)
	for alignedread in samfile:
		txid = str(alignedread.qname)
		mytid = alignedread.tid
		chr = samfile.getrname(mytid)
		start = alignedread.pos
		keys = []
		values = []
		for tag in alignedread.tags:
			keys.append(tag[0])
			values.append(tag[1])
		tagdict = dict(zip(keys, values))
		try:
			strand = tagdict['XS']
		except:
			strand = "strand error"
		TX[txid]['chr'] = chr
		TX[txid]['strand'] = strand
		TX[txid]['start'] = start
		TX[txid]['chain'] = alignedread.cigar
	return TX


def tab_to_dict(tabfile):
	"""
	Generic make a dict from a table
	Assumes first column has key
	and there are column headers
	"""
	mytab = {}
	lines = csv.reader(open(tabfile, 'rb'), delimiter='\t')
	linecount = 0
	for line in lines:
		if (linecount < 1):
			"""
			First line is column header - use as keys
			"""
			keys = line
		else: 
			values = line
			linedict = dict(zip(keys, values))
			id = str(values[0])
			mytab[id] = linedict 
			#print "adding to ",linedict['juncid']
			#print linedict
		linecount += 1
	return mytab

def parse_options():
	parser = argparse.ArgumentParser(description='Parse a bam file.')
	parser.add_argument('-i', help='bam file name', action="store", dest="i")
	parser.add_argument('-o', help='outfile', action="store", dest="o", default="./sims_out/")
	parser.add_argument('-g', help='referenceGTF', action="store", dest="g")
	parser.add_argument('-t', help='transcript abundances file', action="store", dest="t", default="")
	parser.add_argument('-f', help='Fasta file', action="store", dest="f")
	parser.add_argument('-cov', help='coverage to sim', action="store", dest="c", type=int)
	parser.add_argument('-rpk', help='rpk to sim', action="store", dest="r", type=int)
	parser.add_argument('-s', help='start selection mode', action="store", dest="s", default="random")
	parser.add_argument('-m', help='model', action="store", dest="m", default="Default")
	parser.add_argument('-bp', help='model', action="store", type=int, dest="bp", default=76)
	args = parser.parse_args()
	return args

# initialize parameters

args = parse_options()
bamfile = args.i
outfile = args.o
gtffile = args.g
fastafile = args.f
mycov = args.c
myrpk = args.r
txinput = args.t
startmodel = args.s
model = args.m
mybp = args.bp
# Prepare output directory
output_dir = outfile
spanky_utils.prepare_output_dir(output_dir)

tx_result_out_name = output_dir + "/transcript_sims.txt"
tx_out = open(tx_result_out_name, "w")

sim_sam_out_name = output_dir + "/sim.sam"
sim_sam_out = open(sim_sam_out_name, "w")

sim_fastq_out_name = output_dir + "/sim.fastq"
sim_fastq_out = open(sim_fastq_out_name, "w")

junc_cov_out_name = output_dir + "/junc_coverage.txt"
junc_cov_out = open(junc_cov_out_name, "w")

log_out_name = output_dir + "/logs/log.txt"
log_out = open(log_out_name, "w")


# Try making juncresults dict global
JUNCRESULTS = collections.defaultdict(lambda : collections.defaultdict(dict))

def main():
	global mycov
	global myrpk


 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Checking that required files are present
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	if not fastafile:
		quit("Need to specify a fasta file")
	if not gtffile:
		quit("Need to specify a fasta file")

 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Checking that model name is correct
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	possible_models = ['Default','NIST','dm3']
 	
 	if model not in possible_models:
 		error_msg =  "Don't recognize model name " +  model
 		quit(error_msg)

 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Writing parameters to log file
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	print >> log_out, "[%s] Run started" % (spanky_utils.timestamp())
	print >> log_out, "Fastafile:", fastafile
	print >> log_out, "Reference GTF file:", gtffile
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Confirming whether to use coverage or rpk
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	
 	if txinput:
 		if (mycov or myrpk):
 			print "Transcript file: ", txinput
 			quit("Can't specify coverage or rpk if you supply a separate transcript file")
 		print >> log_out, "Input transcript file:", txinput
 	elif (mycov and myrpk):
 		quit("Can't specify both coverage and rpk")
 	elif (mycov or myrpk):
 		if mycov:
 			abundance_metric = "Coverage"
 			abundance_value = mycov
  		if myrpk:
 			abundance_metric = "RPK"
 			abundance_value = myrpk
 		print >> log_out, abundance_metric + ":", abundance_value
 	else:
 		print "Using default, coverage = 1 for all transcripts"
 		abundance_metric = "Coverage"
 		abundance_value = 1
 		mycov = 1
 		print >> log_out, abundance_metric + ":", abundance_value

 			

 	#~~~~~~~~~~~~~~~~~~~
	# Load table of tx abundances
	# to sim
	#~~~~~~~~~~~~~~~~~~~
	if txinput:
		txtab = tab_to_dict(txinput)
	
	#if txinput:
	#	for i in txtab.keys():
	#		print i, txtab[i]

 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Load a fasta file
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	
	print >> sys.stderr, "[%s] Trying to load fasta %s" % (spanky_utils.timestamp(), output_dir)
	f = Fasta(fastafile)
	print >> sys.stderr, "[%s] Done loading fasta %s" % (spanky_utils.timestamp(), output_dir)
	

 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Intializing the reference
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# You need the gtf file, and the fasta file
 	#global lookup
 	#lookup = prep_ref_sub.prep_ref(gtffile,fastafile)
  	## Note that you now have a reference called ref.bam, and a lookup dict
 	#reffile = "ref.bam"

	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Intializing the reference
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# You need the gtf file, and the fasta file
 	lookup = spanky_utils.prep_ref(gtffile,fastafile,output_dir)
  	## Note that you now have a reference called ref.bam, and a lookup dict
	tmp_dir = output_dir + "/tmp/"
 	reffile = tmp_dir + "/ref.bam"

 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Load an annotation, flattened as bam (Spanky way)
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	print >> sys.stderr, "[%s] Loading annotation as bam" % (spanky_utils.timestamp())
 	reffh = pysam.Samfile( reffile, "rb" )
	#edgedict, refjuncs = spanky_parse_utils.parseRefAsBam(reffh)
	
	if txinput:
		edgedict, refjuncs = spanky_parse_utils.parseRefSelectionAsBam(reffh,txtab.keys())
	else:
		edgedict, refjuncs = spanky_parse_utils.parseRefAsBam(reffh)

	refjuncs.sort()
	reffh.close()
	reffh = pysam.Samfile( reffile, "rb" )
	TX = reference_transcript_dict(reffh)
	reffh.close()
	print >> sys.stderr, "[%s] Done loading annotation as bam" % (spanky_utils.timestamp())

 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Load an annotation, flattened as bam
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	#print >> sys.stderr, "[%s] Trying to load annotation as bam %s" % (right_now(), output_dir)
	#TX = parseRefAsBam3(reffile)
	#print >> sys.stderr, "[%s] Done loading annotation as bam %s" % (right_now(), output_dir)
	
	reftx = TX.keys()



	# Now limit to just those in the TX table
	if txinput:
		reftx = txtab.keys()
	# (Need to do the same to the refjunctable!)

	for junc in refjuncs:
		JUNCRESULTS[junc]['totalcov'] = 0
		JUNCRESULTS[junc]['effectivecov'] = 0
	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	# Parameters of sim
	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	#mycov = cov
	intron_retention = 0.20

	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	# Models
	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	'''
	Sequence-based coverage heterogeneity
	Starting point weights based on sequence
	'''
	mers = 7
	seqweights = {}
	counter = 1
	for p in itertools.product('ACGT', repeat=mers):
		## Weight how they are lexicographically sorted
		seqweights[''.join(p)] = round(counter/(4**mers) * 100,4) 
		counter += 1

	"""
	Note for other model attributes, it can be coded so that
	different model types are used for each.  For now, only
	one model type is allowed
	"""

	"""
	Total mismatches
	MMNUM
	"""
	
	mmprob = models.getMMNUMmodel(model,mybp)
	
	"""
	Positions of mismatches
	MMPOS
	"""
	mmposprob = models.getMMPOSmodel(model,mybp)
	"""
	Type of mismatches
	MMTYPE
	(Currently just doing random base sub.)
	"""
	mmtypeprob = models.getMMTYPEmodel(model)


	"""
	Quality scores
	QUAL
	"""

	# consensus quality string
	
	qualstring = models.getQUALmodel(model,mybp)

	# consensus quality score at mismatches
	
	mmqual= models.getMMQUALmodel(model)
	
	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	# counters
	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	txcount = 0
	txskip = 0


	##################################
	# Iterate over each transcript
	##################################

	print >> tx_out, "counter\ttxid\tcov\trpk\tnumreads\tlength\tstrand"


	#for i in range(numtx):
	counter = len(reftx)
	for i in reftx:
	
		#print "On tx number ", i + 1
		txcount += 1
		mytx = i

		#print "simming", mytx

		#mytx = "FBtr0078170"
		#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		# Get tx sequence
		#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
		'''
		Note it comes back revcomped if minus strand
		'''
		myseq_spliced, myseq_unspliced = getTxSequence(TX[mytx],f)
		spllength = len(myseq_spliced)
		prelength = len(myseq_unspliced)

		'''
		getting starting position weights
		
		'''
		startweights_spliced = getStartweights(str(myseq_spliced),mers,seqweights)
		startweights_unspliced = getStartweights(str(myseq_unspliced),mers,seqweights)
	
		#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		# Set up indexes to keep track of position, strand
		#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
		'''
		range:  start, end
		using zero-based start
		'''
		pre2genome = range(TX[mytx]['start'],TX[mytx]['start'] + prelength)
	
		#print TX[mytx]['chain']
		
	
		precigar = explodingCigar(TX[mytx]['chain'])
		# Getting an index of GENOMIC coordinates
		#coordidx, splicedcoordidx, indexstring = genomeCoordString(TX[mytx]['chain'],TX[mytx]['start'])
		#pre2genome, spl2pre = genomeCoordString(TX[mytx]['chain'],TX[mytx]['start'])
		spl2pre = makePreRNAindex(TX[mytx]['chain'],TX[mytx]['start'])

		strandidx = []
		strandidx.append(TX[mytx]['strand'])
		if strandidx[0] == "+":
			strandidx.append("-")
		else:
			strandidx.append("+")
			myseq_spliced = myseq_spliced.reverse_complement()
			myseq_unspliced = myseq_unspliced.reverse_complement()

		#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		# Find number of reads required
		#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		
		#Coverage is the average number of reads representing a given nucleotide
		#in the reconstructed sequence. It can be calculated from the length of the
		#original genome (G), the number of reads(N), and the average read length(L)
		#as NL / G. For example, a hypothetical genome with 2,000 base pairs 
		#reconstructed from 8 reads with an average length of 500 nucleotides will
		#have 2x redundancy.
	
		# Have two options: calculate reads for all trancripts at teh same level
		# - coverage
		# - rpk
		# Input an optional file of transcripts and weights
		# Sim all transcripts or just those in the weights table
		
		# The number of reads simmed, for 10x cov of a 1kb transcript, wiht 76bp se reads
		# is 132 reads
	
		# To calculate reads from the other data points:
		# N = (Cov * G) / L
		# coverage * G = NL
		# cov = NL/G
	
		###!!! NOTE 
		### The way I do it now, you should not cut the number of reads in half for PE!.
		
		#covtimeslength = mycov * len(myseq_spliced);
		#numreads = int(covtimeslength / 76);	
		#if isodd(numreads): numreads += 1 #(Rounds up to even number)
		
		# Always calculated RPK and Coverge, no matter how you decide how many reads to make
		
		if mycov:
			covtimeslength = mycov * len(myseq_spliced)
			numreads = int(covtimeslength / mybp)
		elif myrpk:
			numreads = int(myrpk * len(myseq_spliced)/1000) 
		elif txtab:
		 	if 'rpk' in txtab[mytx].keys():
		 		abundance_metric = "RPK"
		 		abundance_value =  int(txtab[i]['rpk'])
		 		#myrpk = int(txtab[mytx]['rpk'])
		 		numreads = int(abundance_value * len(myseq_spliced)/1000)
		 	elif 'cov' in txtab[i].keys():
		 		abundance_metric = "Coverage"
 				abundance_value = int(txtab[mytx]['cov'])
		 		#mycov = int(txtab[mytx]['cov'])
		 		covtimeslength = abundance_value * len(myseq_spliced)
				numreads = int(covtimeslength / mybp)
			else:
		 		print "Don't recognize txtab headings", txtab[i].keys()
		else:
			quit("Can't figure out how many reads to generate")
		

		if isodd(numreads): numreads += 1 #(Rounds up to even number)

		# Get spliced/unspliced number of reads
		numreads_unspliced = int(intron_retention * numreads)
		numreads_spliced = numreads  - numreads_unspliced
	
		if spllength < 100: 
			numreads = 0
			numreads_spliced = 0
			numreads_unspliced = 0
			txskip += 1
		#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		# Calculate RPK, print answers table
		#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
		RPK = numreads / len(myseq_spliced) * 1000;
		COV = (numreads * mybp) / len(myseq_spliced)
		
		print counter, mytx, COV, RPK, numreads, len(myseq_spliced), strandidx[0]
		print >> tx_out, counter, mytx, COV, RPK, numreads, len(myseq_spliced), strandidx[0]
		counter -= 1
	
		#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		# Generate reads
		#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		#print startweights_spliced	
		for x in range(0,numreads_spliced):
  			myseq = myseq_spliced

			mycigar = precigar		
			mycoords = pre2genome
			mystrand = strandidx[0]
			myspl2pre = spl2pre
			
			if (startmodel == "random"):
				readtuple, samline = simRead(mytx,x, myseq, mycigar, mycoords, strandidx, mybp, mmprob, mmposprob, TX, myspl2pre, f, refjuncs, qualstring, mmqual)
			elif (startmodel == "weighted"):
				readtuple, samline = simRead_weightstart(mytx,x, myseq, mycigar, mycoords, strandidx, mybp, mmprob, mmposprob, TX, myspl2pre, f, refjuncs, qualstring, mmqual,startweights_spliced)
			else:
				quit("Don't recognize start model")

			for read in readtuple:
				print >> sim_fastq_out,  read
			
			print >> sim_sam_out, '\t'.join(samline)

	
		for x in range(0,numreads_unspliced):
			myseq = myseq_unspliced
			mycigar = "M" * len(myseq)
			mycoords = pre2genome
			myspl2pre = range(0,len(myseq))
			mystrand = strandidx[0]
			
			if (startmodel == "random"):
				readtuple, samline = simRead(mytx,x, myseq, mycigar, mycoords, strandidx, mybp, mmprob, mmposprob, TX, myspl2pre, f, refjuncs, qualstring, mmqual)
			elif (startmodel == "weighted"):
				readtuple, samline = simRead_weightstart(mytx,x, myseq, mycigar, mycoords, strandidx, mybp, mmprob, mmposprob, TX, myspl2pre, f, refjuncs, qualstring, mmqual,startweights_unspliced)
			else:
				quit("Don't recognize start model")			

			for read in readtuple:
				print >> sim_fastq_out,  read
			
			print >> sim_sam_out, '\t'.join(samline)
	
	
	print >> sys.stderr, "[%s] Done with sim %s" % (spanky_utils.timestamp(), output_dir)
	print txcount, "transcripts simmed"
	print txskip, "transcripts skipped"
	print len(reftx), "transcripts in ref"
	print "used", startmodel, "model for start selection"
	print >> junc_cov_out, "juncid\ttotal_cov\teffective_cov"
	for junc in refjuncs:
		print >> junc_cov_out, junc, JUNCRESULTS[junc]['totalcov'], JUNCRESULTS[junc]['effectivecov']
	
	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	# Convert simulated SAM file to BAM
	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	sim_sam_out.close()

	
	samfile_prefix = "sim"
	spanky_utils.sam_to_bam(samfile_prefix,fastafile,output_dir)

if __name__ == "__main__":
    sys.exit(main())





